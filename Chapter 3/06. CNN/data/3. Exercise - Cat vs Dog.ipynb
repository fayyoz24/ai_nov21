{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prescription-footwear",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "    <h2 align=\"center\"> Cats vs Dogs </h2>\n",
    "    <h2 align=\"center\" style=\"color:#01ff84\"> Loading your dataset</h2>\n",
    "<div>\n",
    "\n",
    "<br>\n",
    "    \n",
    "So far we've been working with fairly artificial datasets that you wouldn't typically be using in real projects. Instead, you'll likely be dealing with full-sized images like you'd get from smart phone cameras. In this notebook, we'll look at how to load images and use them to train neural networks.\n",
    "\n",
    "We'll be using a [dataset of cat and dog photos](https://www.kaggle.com/c/dogs-vs-cats) available from Kaggle. Here are a couple example images:\n",
    "\n",
    "<img src=\"imgs/dog.png\" width=\"200\" height=\"40\" />\n",
    "<img src=\"imgs/cat.png\" width=\"200\" height=\"40\" />\n",
    "    \n",
    "\n",
    "We'll use this dataset to train a neural network that can differentiate between cats and dogs. These days it doesn't seem like a big accomplishment, but five years ago it was a serious challenge for computer vision systems.\n",
    "    \n",
    "<br>\n",
    "    \n",
    "Start importing the needed libraries:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "major-scanner",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fayyo\\.conda\\envs\\deep_learning\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\fayyo\\.conda\\envs\\deep_learning\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-spiritual",
   "metadata": {},
   "source": [
    "The easiest way to load image data is with `datasets.ImageFolder` from `torchvision` ([documentation](http://pytorch.org/docs/master/torchvision/datasets.html#imagefolder)). In general you'll use `ImageFolder` like so:\n",
    "\n",
    "```python\n",
    "dataset = datasets.ImageFolder('path/to/data', transform=transform)\n",
    "```\n",
    "\n",
    "where `'path/to/data'` is the file path to the data directory and `transform` is a list of processing steps built with the [`transforms`](http://pytorch.org/docs/master/torchvision/transforms.html) module from `torchvision`. ImageFolder expects the files and directories to be constructed like so:\n",
    "```\n",
    "root/dog/xxx.png\n",
    "root/dog/xxy.png\n",
    "root/dog/xxz.png\n",
    "\n",
    "root/cat/123.png\n",
    "root/cat/nsdf3.png\n",
    "root/cat/asd932_.png\n",
    "```\n",
    "\n",
    "where each class has it's own directory (`cat` and `dog`) for the images. The images are then labeled with the class taken from the directory name. So here, the image `123.png` would be loaded with the class label `cat`. You can download the dataset already structured like this [from here](https://s3.amazonaws.com/content.udacity-data.com/nd089/Cat_Dog_data.zip). They are already splitted into a training set and test set.\n",
    "\n",
    " \n",
    ">**Exercise:** Download the dataset and place the train and test set in the `datasets/cat_vs_dog` folder. If you're cloning this from github, you should have it in `../datasets/`. So first create the `cat_vs_dog` folder in `datasets` and verify that the data are there by running `ls ../datasets/cat_vs_dog` (or your custom path if you changed it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-batman",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "interesting-lodging",
   "metadata": {},
   "source": [
    "Great! Now that you have downloaded your data, you need to define the transformations to be passed to the `ImageFolder` function. You have already used them with the MNIST dataset (see the Data Augmentation workbook in Pytorch). While for MNIST you were passing the transformation in the following line of code\n",
    "\n",
    "`datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)`\n",
    "\n",
    "here you do it in the `ImageFolder` method. You can think it as a way to work on every dataset, not only on the MNIST one.\n",
    "\n",
    "\n",
    "### Transforms\n",
    "\n",
    "When you load in the data with `ImageFolder`, you'll need to define some transforms. For example, the images are different sizes but we'll need them to all be the same size for training. You can either resize them with `transforms.Resize()` or crop with `transforms.CenterCrop()`, `transforms.RandomResizedCrop()`, etc. We'll also need to convert the images to PyTorch tensors with `transforms.ToTensor()`. Typically you'll combine these transforms into a pipeline with `transforms.Compose()`, which accepts a list of transforms and runs them in sequence. \n",
    "\n",
    "As in the other notebook, you can use the following transformations:\n",
    "\n",
    "```python\n",
    "transform = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.5, 0.5, 0.5], \n",
    "                                                            [0.5, 0.5, 0.5])])\n",
    "\n",
    "```\n",
    "\n",
    "**WARNING!** Remember that transformation are super useful for \"augmenting\" your training data, so that you make your network less vulnerable to different sizes, rotations, or cropping. However, when you are on the test data, there is no need of augmenting the data! Actually, it is not a good practice to do that because there would be very repetitive test data that invalidates your score.\n",
    "\n",
    "For this reason, define two different transformations for training and test data (remember that `ToTensor()` and normalization are necessary also for the test data, as well as the resizing (you can use `transforms.Resize(size)` for it):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "limiting-smoke",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any class folder in train.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fayyo\\OneDrive\\Documents\\GitHub\\ai_nov211\\Chapter 3\\06. CNN\\data\\3. Exercise - Cat vs Dog.ipynb Cell 6'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fayyo/OneDrive/Documents/GitHub/ai_nov211/Chapter%203/06.%20CNN/data/3.%20Exercise%20-%20Cat%20vs%20Dog.ipynb#ch0000005?line=0'>1</a>\u001b[0m \u001b[39m# train_transform = # your code here\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fayyo/OneDrive/Documents/GitHub/ai_nov211/Chapter%203/06.%20CNN/data/3.%20Exercise%20-%20Cat%20vs%20Dog.ipynb#ch0000005?line=1'>2</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fayyo/OneDrive/Documents/GitHub/ai_nov211/Chapter%203/06.%20CNN/data/3.%20Exercise%20-%20Cat%20vs%20Dog.ipynb#ch0000005?line=2'>3</a>\u001b[0m \u001b[39m# test_transform = # your code here\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fayyo/OneDrive/Documents/GitHub/ai_nov211/Chapter%203/06.%20CNN/data/3.%20Exercise%20-%20Cat%20vs%20Dog.ipynb#ch0000005?line=3'>4</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fayyo/OneDrive/Documents/GitHub/ai_nov211/Chapter%203/06.%20CNN/data/3.%20Exercise%20-%20Cat%20vs%20Dog.ipynb#ch0000005?line=4'>5</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fayyo/OneDrive/Documents/GitHub/ai_nov211/Chapter%203/06.%20CNN/data/3.%20Exercise%20-%20Cat%20vs%20Dog.ipynb#ch0000005?line=5'>6</a>\u001b[0m \u001b[39m# data_dir = \"./C:/Users/fayyo/OneDrive/Documents/GitHub/ai_nov211/Chapter 3/06. CNN/data\" # or the path where you have downloaded the dataset\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fayyo/OneDrive/Documents/GitHub/ai_nov211/Chapter%203/06.%20CNN/data/3.%20Exercise%20-%20Cat%20vs%20Dog.ipynb#ch0000005?line=7'>8</a>\u001b[0m train_data \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mImageFolder(\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\deep_learning\\lib\\site-packages\\torchvision\\datasets\\folder.py:310\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=301'>302</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=302'>303</a>\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=303'>304</a>\u001b[0m         root: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=307'>308</a>\u001b[0m         is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=308'>309</a>\u001b[0m ):\n\u001b[1;32m--> <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=309'>310</a>\u001b[0m     \u001b[39msuper\u001b[39;49m(ImageFolder, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(root, loader, IMG_EXTENSIONS \u001b[39mif\u001b[39;49;00m is_valid_file \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=310'>311</a>\u001b[0m                                       transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=311'>312</a>\u001b[0m                                       target_transform\u001b[39m=\u001b[39;49mtarget_transform,\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=312'>313</a>\u001b[0m                                       is_valid_file\u001b[39m=\u001b[39;49mis_valid_file)\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=313'>314</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples\n",
      "File \u001b[1;32m~\\.conda\\envs\\deep_learning\\lib\\site-packages\\torchvision\\datasets\\folder.py:145\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=133'>134</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=134'>135</a>\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=135'>136</a>\u001b[0m         root: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=140'>141</a>\u001b[0m         is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=141'>142</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=142'>143</a>\u001b[0m     \u001b[39msuper\u001b[39m(DatasetFolder, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(root, transform\u001b[39m=\u001b[39mtransform,\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=143'>144</a>\u001b[0m                                         target_transform\u001b[39m=\u001b[39mtarget_transform)\n\u001b[1;32m--> <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=144'>145</a>\u001b[0m     classes, class_to_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_classes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot)\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=145'>146</a>\u001b[0m     samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_dataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=147'>148</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader \u001b[39m=\u001b[39m loader\n",
      "File \u001b[1;32m~\\.conda\\envs\\deep_learning\\lib\\site-packages\\torchvision\\datasets\\folder.py:221\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=193'>194</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(\u001b[39mself\u001b[39m, directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=194'>195</a>\u001b[0m     \u001b[39m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=195'>196</a>\u001b[0m \n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=196'>197</a>\u001b[0m \u001b[39m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=218'>219</a>\u001b[0m \u001b[39m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=219'>220</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=220'>221</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m find_classes(directory)\n",
      "File \u001b[1;32m~\\.conda\\envs\\deep_learning\\lib\\site-packages\\torchvision\\datasets\\folder.py:42\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=39'>40</a>\u001b[0m classes \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(entry\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mscandir(directory) \u001b[39mif\u001b[39;00m entry\u001b[39m.\u001b[39mis_dir())\n\u001b[0;32m     <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=40'>41</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[1;32m---> <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=41'>42</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find any class folder in \u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=43'>44</a>\u001b[0m class_to_idx \u001b[39m=\u001b[39m {cls_name: i \u001b[39mfor\u001b[39;00m i, cls_name \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(classes)}\n\u001b[0;32m     <a href='file:///~/.conda/envs/deep_learning/lib/site-packages/torchvision/datasets/folder.py?line=44'>45</a>\u001b[0m \u001b[39mreturn\u001b[39;00m classes, class_to_idx\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in train."
     ]
    }
   ],
   "source": [
    "# train_transform = # your code here\n",
    "\n",
    "# test_transform = # your code here\n",
    "\n",
    "\n",
    "# data_dir = \"./C:/Users/fayyo/OneDrive/Documents/GitHub/ai_nov211/Chapter 3/06. CNN/data\" # or the path where you have downloaded the dataset\n",
    "\n",
    "train_data = datasets.ImageFolder('train')#, transform=train_transforms)\n",
    "# test_data = datasets.ImageFolder(data_dir + '/test')#, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ce3c50",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "likely-cleaners",
   "metadata": {},
   "source": [
    "Now that you have you have defined the needed transformation, it's time to build the Data loader itself!\n",
    "\n",
    "### Data Loaders\n",
    "\n",
    "With the `ImageFolder` loaded, you have to pass it to a [`DataLoader`](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader). The `DataLoader` takes a dataset (such as you would get from `ImageFolder`) and returns batches of images and the corresponding labels. You can set various parameters like the batch size and if the data is shuffled after each epoch.\n",
    "\n",
    "```python\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "```\n",
    "\n",
    "Here `dataloader` is a [generator](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/). To get data out of it, you need to loop through it or convert it to an iterator and call `next()`.\n",
    "\n",
    "```python\n",
    "# Looping through it, get a batch on each loop \n",
    "for images, labels in dataloader:\n",
    "    pass\n",
    "\n",
    "# Get one batch\n",
    "images, labels = next(iter(dataloader))\n",
    "```\n",
    " \n",
    ">**Exercise:** Build the dataloader for both the train and test data. Choose the batch size that fits your memory. \n",
    "**Remember** NOT TO shuffle the test data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader =  # your code here\n",
    "testloader = # your code here\n",
    "trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "# Run this to test your data loaders\n",
    "images, labels = next(iter(trainloader))\n",
    "imshow(images[0], normalize=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "premium-donna",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fayyo\\OneDrive\\Documents\\GitHub\\ai_nov211\\Chapter 3\\06. CNN\\data\\3. Exercise - Cat vs Dog.ipynb Cell 11'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fayyo/OneDrive/Documents/GitHub/ai_nov211/Chapter%203/06.%20CNN/data/3.%20Exercise%20-%20Cat%20vs%20Dog.ipynb#ch0000010?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-ownership",
   "metadata": {},
   "source": [
    "Ok, now let's create a simple Convolutional Neural Network for this task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-folks",
   "metadata": {},
   "source": [
    "**N.B.** When building a convolutional neural network, be careful for making all the shapes to match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-giant",
   "metadata": {},
   "source": [
    "Why do we reshape in the middle of the forward pass?\n",
    "\n",
    "As you remember from the class, the output of a convolutional layer is always a 3D volume! For this reason, since the output channels of the conv2 layer is 16 and the feature maps have size 5x5, then the input of the fc1 layer must be reshaped to have shape `(batch_size, 16 * 5 * 5)`.\n",
    "\n",
    "Now the question is: what will be the difference in the training of this network with respect to the fully-connected one you are used to?\n",
    "\n",
    "None, except for the fact that you do not reshape the input to be a vector, but you keep the shape as a volume! \n",
    "\n",
    ">**Exercise:** Implement a Convolutional Neural Network for the cat vs dog challenge, such that:\n",
    "> - The input images have shape 28x28 and three RGB channels\n",
    "> - You have 2 Conv2d layer with MaxPool2D in the middle and two fully-connected layer at the end.\n",
    "> - You can decide yourself the rest of the hyperparameters (kernel size, number of filters...)\n",
    "> - Train and evaluate your model\n",
    "\n",
    "Following there's an helper function to visualize your prediction once your model has been built.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class_list = train_data.classes\n",
    "\n",
    "def view_classify_general(img, ps, class_list):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    imshow(img, ax=ax1, normalize=True)\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(len(class_list)), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(len(class_list)))\n",
    "    ax2.set_yticklabels([x for x in class_list], size='small');\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "img, label = images[0], labels[0]\n",
    "# Flatten images\n",
    "# Forward pass, get our logits\n",
    "logits = net(img.view(1, *images[0].shape))\n",
    "# Calculate the loss with the logits and the labels\n",
    "ps = torch.exp(logits)\n",
    "    \n",
    "view_classify_general(img, ps, class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-cylinder",
   "metadata": {},
   "source": [
    "You should get something cute like this:\n",
    "\n",
    "![image](imgs/cat_pred.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b131c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
